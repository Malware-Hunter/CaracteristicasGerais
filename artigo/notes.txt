A partir desse momento, nosso objetivo se torna construir nossa árvore tendo o conjunto de dados inteiro como raiz e criar ramificações
baseadas em condições que minimizem a entropia e aumentem o ganho de informação. 
O valor da entropia de um dado pode ser calculado pela seguinte fórmula: ...

A ideia por trás da construção de árvores é encontrar a melhor característica para dividir que gere o maior ganho de informação ou forneça a menor
incerteza nas folhas seguintes

Entropia
pode ser definida como a medida que nos diz o quanto nossos dados
estão desorganizados e misturados. Quanto maior a entropia, menor o ganho de informação e vice-versa.
Nossos dados ficam menos entrópicos conforme dividimos os dados em conjuntos capazes de representar apenas uma classe do nosso modelo

Ainda encontraremos o ganho de informação, usando entropias ponderadas e escolheremos o atributo que proporcionou o ganho máximo de informação.